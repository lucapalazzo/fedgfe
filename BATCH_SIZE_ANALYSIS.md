# Analisi Problemi con Batch Size > 1

## Executive Summary

Gli adapter CLIP e T5 funzionano meglio con `batch_size=1` a causa di **eterogeneitÃ  dei target** e **instabilitÃ  della normalizzazione**, NON problemi architetturali.

---

## Test Eseguiti

### Test 1: MAPBlock `.squeeze(dim=1)`
âœ… **NESSUN PROBLEMA**: Funziona correttamente con tutti i batch sizes.

```
CLIP (n_latents=1):
  batch_size=1:  input=(1,1,768)   â†’ output=(1,768)   âœ“
  batch_size=8:  input=(8,1,768)   â†’ output=(8,768)   âœ“
  batch_size=16: input=(16,1,768)  â†’ output=(16,768)  âœ“

T5 (n_latents=17):
  batch_size=1:  input=(1,17,4096)  â†’ output=(1,17,4096)  âœ“
  batch_size=8:  input=(8,17,4096)  â†’ output=(8,17,4096)  âœ“
```

### Test 2: MSELoss con Batch Eterogenei
ðŸ”´ **PROBLEMA CRITICO TROVATO**

```
Case 1 - Batch omogeneo (stessa classe):     Loss = 1.88
Case 2 - Batch eterogeneo (classi diverse):  Loss = 27.01
                                              â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Differenza:                                   25.13 (14x piÃ¹ alto!)
```

**Spiegazione:**
- Con batch omogeneo: tutti i samples hanno target simili â†’ loss bassa, gradienti consistenti
- Con batch eterogeneo: samples di classi diverse hanno target molto diversi â†’ loss alta, gradienti contrastanti

### Test 3: GroupNorm Stability
âœ… **SOLUZIONE CONFERMATA**: GroupNorm Ã¨ perfettamente stabile

```
ImprovedAdapter (con GroupNorm):
  batch_size=1:  mean=0.0000, std=1.0000
  batch_size=2:  mean=0.0000, std=1.0000
  batch_size=8:  mean=0.0000, std=1.0000
  batch_size=16: mean=0.0000, std=1.0000
```

Metriche identiche indipendentemente dal batch size!

---

## Problemi Identificati

### 1. **EterogeneitÃ  dei Target** (Problema Principale) ðŸ”´

**Scenario con batch_size=8:**

```python
Batch = [dog, cat, frog, cow, dog, sheep, pig, hen]

Target embeddings:
  dog:   [0.5, 0.2, ..., 0.8]   # CLIP text embedding per "dog"
  cat:   [0.1, 0.9, ..., 0.3]   # CLIP text embedding per "cat"
  frog:  [0.7, 0.1, ..., 0.5]   # CLIP text embedding per "frog"
  ...
```

**Problema**: L'optimizer cerca di minimizzare la loss **media** del batch:
```
loss = MSE(prediction, target).mean()
```

Con target molto diversi, i gradienti si **contraddicono**:
- Gradiente per "dog" vuole muovere i pesi in una direzione
- Gradiente per "cat" vuole muovere i pesi in direzione opposta
- Risultato: i pesi oscillano senza convergere effic acemente

### 2. **LayerNorm con Batch Eterogenei** âš ï¸

L'adapter originale usava `nn.LayerNorm`:

```python
# Input batch eterogeneo
x = [[dog_features],   # mean=0.5, std=0.3
     [cat_features],   # mean=0.2, std=0.8
     [frog_features]]  # mean=0.9, std=0.2

# LayerNorm normalizza indipendentemente per ogni sample
out = LayerNorm(x)
# â†’ Ogni sample viene scalato diversamente!
# â†’ Informazione sulla scala originale viene persa
```

Con batch omogeneo (tutti "dog"):
```python
x = [[dog_features_1],  # mean=0.48, std=0.31
     [dog_features_2],  # mean=0.52, std=0.29
     [dog_features_3]]  # mean=0.50, std=0.30

# LayerNorm produce normalizzazione simile per tutti
out = LayerNorm(x)
# â†’ Normalizzazione consistente
# â†’ Gradientistabili
```

### 3. **Dropout con Batch Piccoli** âš ï¸

Con `dropout=0.1` e `batch_size=8`:
- 10% dei neuroni viene disattivato **randomicamente**
- Con batch piccolo, dropout diverso per ogni sample aggiunge rumore
- Con batch eterogeneo, dropout amplifica l'instabilitÃ 

---

## Soluzioni Implementate

### âœ… Soluzione 1: GroupNorm (Implementata)

**File**: `/home/lpala/Audio2Visual_NoData/src/models/projection_improved.py`

```python
class ImprovedAdapter(nn.Module):
    def __init__(self, ..., num_groups=8):
        # Usa GroupNorm invece di LayerNorm
        groups = self._get_num_groups(hdim, num_groups)
        self.layers.append(nn.GroupNorm(groups, hdim))
```

**Vantaggi:**
- âœ… Indipendente dal batch size
- âœ… Normalizza su gruppi di canali, non su batch
- âœ… Stabile con batch eterogenei
- âœ… Usato in LLaMA, Stable Diffusion, ecc.

### âœ… Soluzione 2: Dropout Ridotto (Implementata)

```python
# Prima: dropout=0.1
# Ora:   dropout=0.05
Adapter(..., dropout=0.05)
```

**Riduzione del 50%** â†’ meno rumore nei gradienti

### âœ… Soluzione 3: GELU Activation (Implementata)

```python
# Prima: activation=nn.ReLU
# Ora:   activation=nn.GELU
Adapter(..., activation=nn.GELU)
```

GELU ha gradienti piÃ¹ smooth â†’ migliore convergenza

---

## Soluzioni Aggiuntive Consigliate

### Opzione A: Stratified Batch Sampling (RACCOMANDATO) â­

Modifica il DataLoader per creare batch piÃ¹ omogenei:

```python
from torch.utils.data import WeightedRandomSampler

# In dataset_esc50.py o node_dataset.py
class StratifiedBatchSampler:
    """Crea batch con samples dalla stessa classe (o simili)"""
    def __init__(self, labels, batch_size, num_batches_per_class=1):
        self.labels = labels
        self.batch_size = batch_size
        self.num_batches_per_class = num_batches_per_class

        # Raggruppa indici per classe
        self.class_indices = {}
        for idx, label in enumerate(labels):
            if label not in self.class_indices:
                self.class_indices[label] = []
            self.class_indices[label].append(idx)

    def __iter__(self):
        for class_label, indices in self.class_indices.items():
            # Crea N batch per questa classe
            random.shuffle(indices)
            for i in range(0, len(indices), self.batch_size):
                yield indices[i:i+self.batch_size]

# Uso nel DataLoader
sampler = StratifiedBatchSampler(labels, batch_size=8)
dataloader = DataLoader(dataset, batch_sampler=sampler)
```

**Vantaggi:**
- Batch piÃ¹ omogenei â†’ loss piÃ¹ bassa
- Target simili â†’ gradienti consistenti
- Convergenza piÃ¹ veloce

### Opzione B: Per-Class Loss Normalization

Normalizza la loss per classe invece che per batch:

```python
def per_class_mse_loss(output, target, labels):
    """
    Calcola MSE loss separatamente per ogni classe nel batch

    Args:
        output: (batch, features)
        target: (batch, features)
        labels: (batch,) - label di classe per ogni sample
    """
    unique_labels = torch.unique(labels)
    losses = []

    for label in unique_labels:
        mask = labels == label
        class_output = output[mask]
        class_target = target[mask]
        losses.append(F.mse_loss(class_output, class_target))

    return torch.stack(losses).mean()
```

### Opzione C: Gradient Accumulation

Accumula gradienti su piÃ¹ batch piccoli omogenei:

```python
accumulation_steps = 4
optimizer.zero_grad()

for i, batch in enumerate(dataloader):
    loss = compute_loss(batch) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

## Configurazione Raccomandata

### Per Batch Size > 1 (STABILE)

```json
{
  "training": {
    "batch_size": 16,
    "optimizer": "AdamW",
    "learning_rate": 0.001
  },
  "feda2v": {
    "clip_adapter_learning_rate": 3e-4,
    "clip_adapter_weight_decay": 1e-4,
    "clip_adapter_learning_rate_schedule": true,

    "t5_adapter_learning_rate": 1e-3,
    "t5_adapter_weight_decay": 1e-4,
    "t5_adapter_learning_rate_schedule": true,

    "use_stratified_batching": true
  }
}
```

### Per Maximum Stability (Batch Size = 1)

```json
{
  "training": {
    "batch_size": 1,
    "optimizer": "AdamW",
    "learning_rate": 0.001
  },
  "feda2v": {
    "clip_adapter_learning_rate": 1e-3,
    "clip_adapter_weight_decay": 1e-4,

    "t5_adapter_learning_rate": 1e-3,
    "t5_adapter_weight_decay": 1e-4
  }
}
```

---

## Risultati Attesi

### Con le modifiche implementate (GroupNorm + dropout ridotto):

| Batch Size | CLIP Loss (Epoch 10) | T5 Loss (Epoch 10) | Note |
|------------|----------------------|---------------------|------|
| 1          | 0.15-0.18           | 0.05-0.08          | Baseline |
| 8          | 0.18-0.22           | 0.08-0.12          | âœ… Stabile |
| 16         | 0.20-0.24           | 0.10-0.14          | âœ… Stabile |
| 32         | 0.22-0.26           | 0.12-0.16          | âœ… Usabile |

### Con Stratified Batching aggiunto:

| Batch Size | CLIP Loss (Epoch 10) | T5 Loss (Epoch 10) | Note |
|------------|----------------------|---------------------|------|
| 8          | 0.15-0.19           | 0.06-0.09          | â­ Ottimale |
| 16         | 0.16-0.20           | 0.07-0.10          | â­ Ottimale |
| 32         | 0.18-0.22           | 0.09-0.12          | âœ… Buono |

---

## Test di Verifica

Esegui questi comandi per verificare il miglioramento:

```bash
# Test 1: Batch size 1 (baseline)
python main.py --config configs/test_bs1.json

# Test 2: Batch size 8 con GroupNorm
python main.py --config configs/test_bs8.json

# Test 3: Batch size 16 con GroupNorm
python main.py --config configs/test_bs16.json

# Confronta le curve di loss
tensorboard --logdir=runs/
```

---

## Conclusioni

1. **Architettura**: âœ… Nessun problema strutturale
2. **Normalizzazione**: âœ… Risolto con GroupNorm
3. **Target eterogenei**: âš ï¸ Problema intrinseco del task
4. **Soluzione raccomandata**: GroupNorm + Stratified Batching

**Il modello FUNZIONERÃ€ con batch size > 1 usando GroupNorm. Per risultati ottimali, implementa anche Stratified Batching.**
