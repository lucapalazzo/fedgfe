{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Checkpoint Analysis\n",
    "\n",
    "This notebook helps you explore and analyze generator checkpoints saved during training.\n",
    "\n",
    "## Features:\n",
    "- Load and inspect checkpoint metadata\n",
    "- Visualize generator architecture\n",
    "- Compare checkpoints across rounds\n",
    "- Analyze training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure checkpoint directory\n",
    "CHECKPOINT_DIR = \"checkpoints/generators\"\n",
    "\n",
    "# You can also specify a specific checkpoint file\n",
    "# CHECKPOINT_FILE = \"checkpoints/generators/client_generator_node0.pt\"\n",
    "\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"Directory exists: {Path(CHECKPOINT_DIR).exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find All Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_checkpoints(checkpoint_dir):\n",
    "    \"\"\"Find all checkpoint files in the directory.\"\"\"\n",
    "    checkpoint_files = glob.glob(f\"{checkpoint_dir}/**/*.pt\", recursive=True)\n",
    "    return sorted(checkpoint_files)\n",
    "\n",
    "checkpoints = find_checkpoints(CHECKPOINT_DIR)\n",
    "\n",
    "print(f\"Found {len(checkpoints)} checkpoint(s):\\n\")\n",
    "for i, ckpt in enumerate(checkpoints, 1):\n",
    "    print(f\"{i}. {ckpt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Inspect Checkpoint Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_metadata(checkpoint_path):\n",
    "    \"\"\"Load checkpoint and extract metadata.\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    metadata = {}\n",
    "    \n",
    "    # Extract metadata fields\n",
    "    metadata_fields = [\n",
    "        'client_id', 'node_id', 'round', 'generator_type', 'generator_granularity',\n",
    "        'generator_key', 'diffusion_type', 'visual_dim', 'input_dim', 'hidden_dim',\n",
    "        'latent_dim', 'sequence_length', 'dataset_name', 'selected_classes',\n",
    "        'generator_classes', 'training_samples', 'training_epochs',\n",
    "        'final_loss', 'timestamp', 'device'\n",
    "    ]\n",
    "    \n",
    "    for field in metadata_fields:\n",
    "        metadata[field] = checkpoint.get(field, None)\n",
    "    \n",
    "    # Check for state dicts\n",
    "    metadata['has_generator_state'] = 'generator_state_dict' in checkpoint\n",
    "    metadata['has_optimizer_state'] = 'optimizer_state_dict' in checkpoint\n",
    "    \n",
    "    # Get state dict info if available\n",
    "    if metadata['has_generator_state']:\n",
    "        state_dict = checkpoint['generator_state_dict']\n",
    "        metadata['num_parameters'] = sum(p.numel() for p in state_dict.values())\n",
    "        metadata['parameter_keys'] = list(state_dict.keys())\n",
    "    \n",
    "    return checkpoint, metadata\n",
    "\n",
    "# Load first checkpoint as example\n",
    "if checkpoints:\n",
    "    checkpoint_path = checkpoints[0]\n",
    "    print(f\"Loading: {checkpoint_path}\\n\")\n",
    "    \n",
    "    checkpoint, metadata = load_checkpoint_metadata(checkpoint_path)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CHECKPOINT METADATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for key, value in metadata.items():\n",
    "        if key != 'parameter_keys':  # Skip long list\n",
    "            print(f\"{key:25s}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze All Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_checkpoints(checkpoint_files):\n",
    "    \"\"\"Analyze all checkpoints and create a summary dataframe.\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for ckpt_file in checkpoint_files:\n",
    "        try:\n",
    "            _, metadata = load_checkpoint_metadata(ckpt_file)\n",
    "            metadata['checkpoint_file'] = Path(ckpt_file).name\n",
    "            records.append(metadata)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {ckpt_file}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "if checkpoints:\n",
    "    df_checkpoints = analyze_all_checkpoints(checkpoints)\n",
    "    \n",
    "    print(\"\\nCheckpoint Summary:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Select key columns to display\n",
    "    display_cols = ['checkpoint_file', 'node_id', 'round', 'generator_type', \n",
    "                    'generator_granularity', 'generator_key', 'final_loss', \n",
    "                    'training_samples', 'num_parameters']\n",
    "    \n",
    "    available_cols = [col for col in display_cols if col in df_checkpoints.columns]\n",
    "    \n",
    "    display(df_checkpoints[available_cols])\n",
    "else:\n",
    "    print(\"No checkpoints found to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Generator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_generator_architecture(checkpoint_path):\n",
    "    \"\"\"Visualize the generator architecture from checkpoint.\"\"\"\n",
    "    checkpoint, metadata = load_checkpoint_metadata(checkpoint_path)\n",
    "    \n",
    "    if not metadata['has_generator_state']:\n",
    "        print(\"No generator state found in checkpoint\")\n",
    "        return\n",
    "    \n",
    "    state_dict = checkpoint['generator_state_dict']\n",
    "    \n",
    "    # Extract layer information\n",
    "    layers = []\n",
    "    layer_sizes = []\n",
    "    \n",
    "    for key, param in state_dict.items():\n",
    "        if 'weight' in key:\n",
    "            layers.append(key)\n",
    "            layer_sizes.append(param.shape)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot 1: Parameter count per layer\n",
    "    param_counts = [param.numel() for param in state_dict.values()]\n",
    "    param_names = list(state_dict.keys())\n",
    "    \n",
    "    ax1.barh(range(len(param_names)), param_counts)\n",
    "    ax1.set_yticks(range(len(param_names)))\n",
    "    ax1.set_yticklabels(param_names, fontsize=8)\n",
    "    ax1.set_xlabel('Number of Parameters')\n",
    "    ax1.set_title('Parameters per Layer')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Architecture summary\n",
    "    architecture_info = [\n",
    "        f\"Generator Type: {metadata.get('generator_type', 'N/A')}\",\n",
    "        f\"Input Dim: {metadata.get('input_dim', 'N/A')}\",\n",
    "        f\"Hidden Dim: {metadata.get('hidden_dim', 'N/A')}\",\n",
    "        f\"Latent Dim: {metadata.get('latent_dim', 'N/A')}\",\n",
    "        f\"Visual Dim: {metadata.get('visual_dim', 'N/A')}\",\n",
    "        f\"Sequence Length: {metadata.get('sequence_length', 'N/A')}\",\n",
    "        f\"\\nTotal Parameters: {metadata.get('num_parameters', 'N/A'):,}\",\n",
    "    ]\n",
    "    \n",
    "    ax2.axis('off')\n",
    "    ax2.text(0.1, 0.9, '\\n'.join(architecture_info), \n",
    "             fontsize=12, verticalalignment='top',\n",
    "             fontfamily='monospace')\n",
    "    ax2.set_title('Architecture Configuration')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTotal parameters: {metadata.get('num_parameters', 'N/A'):,}\")\n",
    "\n",
    "if checkpoints:\n",
    "    visualize_generator_architecture(checkpoints[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Checkpoints Across Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(df_checkpoints):\n",
    "    \"\"\"Plot training progress across rounds.\"\"\"\n",
    "    if 'round' not in df_checkpoints.columns or 'final_loss' not in df_checkpoints.columns:\n",
    "        print(\"Missing 'round' or 'final_loss' information in checkpoints\")\n",
    "        return\n",
    "    \n",
    "    # Filter out None values\n",
    "    df_plot = df_checkpoints[df_checkpoints['round'].notna() & df_checkpoints['final_loss'].notna()].copy()\n",
    "    \n",
    "    if df_plot.empty:\n",
    "        print(\"No valid data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Loss over rounds\n",
    "    if 'node_id' in df_plot.columns:\n",
    "        for node_id in df_plot['node_id'].unique():\n",
    "            node_data = df_plot[df_plot['node_id'] == node_id].sort_values('round')\n",
    "            axes[0].plot(node_data['round'], node_data['final_loss'], \n",
    "                        marker='o', label=f'Node {node_id}')\n",
    "    else:\n",
    "        df_plot_sorted = df_plot.sort_values('round')\n",
    "        axes[0].plot(df_plot_sorted['round'], df_plot_sorted['final_loss'], \n",
    "                    marker='o', label='Generator')\n",
    "    \n",
    "    axes[0].set_xlabel('Round')\n",
    "    axes[0].set_ylabel('Final Loss')\n",
    "    axes[0].set_title('Training Loss Over Rounds')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Loss distribution by round\n",
    "    if len(df_plot) > 1:\n",
    "        df_plot.boxplot(column='final_loss', by='round', ax=axes[1])\n",
    "        axes[1].set_xlabel('Round')\n",
    "        axes[1].set_ylabel('Final Loss')\n",
    "        axes[1].set_title('Loss Distribution by Round')\n",
    "        plt.suptitle('')  # Remove automatic title\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'Not enough data\\nfor distribution plot', \n",
    "                    ha='center', va='center', fontsize=12)\n",
    "        axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if checkpoints and 'df_checkpoints' in locals():\n",
    "    plot_training_progress(df_checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Generator Model from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add system path to import generator classes\n",
    "import sys\n",
    "sys.path.append('/home/lpala/fedgfe/system')\n",
    "\n",
    "from flcore.trainmodel.generators import ConditionedVAEGenerator, VAELoss\n",
    "\n",
    "def load_generator_model(checkpoint_path):\n",
    "    \"\"\"Load generator model from checkpoint.\"\"\"\n",
    "    checkpoint, metadata = load_checkpoint_metadata(checkpoint_path)\n",
    "    \n",
    "    if not metadata['has_generator_state']:\n",
    "        print(\"No generator state found in checkpoint\")\n",
    "        return None\n",
    "    \n",
    "    # Create generator with same configuration\n",
    "    generator = ConditionedVAEGenerator(\n",
    "        input_dim=metadata.get('input_dim', 768),\n",
    "        hidden_dim=metadata.get('hidden_dim', 1024),\n",
    "        latent_dim=metadata.get('latent_dim', 256),\n",
    "        visual_dim=metadata.get('visual_dim', 4864),\n",
    "        sequence_length=metadata.get('sequence_length', 4)\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    generator.eval()\n",
    "    \n",
    "    print(f\"✓ Generator loaded from {Path(checkpoint_path).name}\")\n",
    "    print(f\"  - Type: {metadata.get('generator_type', 'N/A')}\")\n",
    "    print(f\"  - Parameters: {metadata.get('num_parameters', 'N/A'):,}\")\n",
    "    print(f\"  - Training samples: {metadata.get('training_samples', 'N/A')}\")\n",
    "    print(f\"  - Final loss: {metadata.get('final_loss', 'N/A')}\")\n",
    "    \n",
    "    return generator, metadata\n",
    "\n",
    "if checkpoints:\n",
    "    generator, gen_metadata = load_generator_model(checkpoints[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Checkpoint Summary to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_checkpoints' in locals() and not df_checkpoints.empty:\n",
    "    output_file = f\"{CHECKPOINT_DIR}/checkpoint_summary.csv\"\n",
    "    \n",
    "    # Select columns to export (exclude complex objects)\n",
    "    export_cols = [col for col in df_checkpoints.columns \n",
    "                   if col not in ['parameter_keys', 'selected_classes', 'generator_classes']]\n",
    "    \n",
    "    df_checkpoints[export_cols].to_csv(output_file, index=False)\n",
    "    print(f\"✓ Checkpoint summary exported to: {output_file}\")\n",
    "else:\n",
    "    print(\"No checkpoint data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Checkpoint Inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_checkpoint_detailed(checkpoint_path):\n",
    "    \"\"\"Detailed inspection of a specific checkpoint.\"\"\"\n",
    "    checkpoint, metadata = load_checkpoint_metadata(checkpoint_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"DETAILED CHECKPOINT INSPECTION: {Path(checkpoint_path).name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Section 1: Node Information\n",
    "    print(\"\\n[1] NODE INFORMATION\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Node ID:          {metadata.get('node_id', 'N/A')}\")\n",
    "    print(f\"Round:            {metadata.get('round', 'N/A')}\")\n",
    "    print(f\"Dataset:          {metadata.get('dataset_name', 'N/A')}\")\n",
    "    print(f\"Timestamp:        {metadata.get('timestamp', 'N/A')}\")\n",
    "    print(f\"Device:           {metadata.get('device', 'N/A')}\")\n",
    "    \n",
    "    # Section 2: Generator Configuration\n",
    "    print(\"\\n[2] GENERATOR CONFIGURATION\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Type:             {metadata.get('generator_type', 'N/A')}\")\n",
    "    print(f\"Granularity:      {metadata.get('generator_granularity', 'N/A')}\")\n",
    "    print(f\"Generator Key:    {metadata.get('generator_key', 'N/A')}\")\n",
    "    print(f\"Diffusion Type:   {metadata.get('diffusion_type', 'N/A')}\")\n",
    "    \n",
    "    # Section 3: Architecture\n",
    "    print(\"\\n[3] ARCHITECTURE\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Input Dim:        {metadata.get('input_dim', 'N/A')}\")\n",
    "    print(f\"Hidden Dim:       {metadata.get('hidden_dim', 'N/A')}\")\n",
    "    print(f\"Latent Dim:       {metadata.get('latent_dim', 'N/A')}\")\n",
    "    print(f\"Visual Dim:       {metadata.get('visual_dim', 'N/A')}\")\n",
    "    print(f\"Sequence Length:  {metadata.get('sequence_length', 'N/A')}\")\n",
    "    print(f\"Total Parameters: {metadata.get('num_parameters', 'N/A'):,}\")\n",
    "    \n",
    "    # Section 4: Training Information\n",
    "    print(\"\\n[4] TRAINING INFORMATION\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Training Samples: {metadata.get('training_samples', 'N/A')}\")\n",
    "    print(f\"Training Epochs:  {metadata.get('training_epochs', 'N/A')}\")\n",
    "    print(f\"Final Loss:       {metadata.get('final_loss', 'N/A')}\")\n",
    "    \n",
    "    # Section 5: Classes\n",
    "    print(\"\\n[5] CLASSES\")\n",
    "    print(\"-\" * 80)\n",
    "    selected_classes = metadata.get('selected_classes', None)\n",
    "    if selected_classes:\n",
    "        print(f\"Selected Classes: {', '.join(map(str, selected_classes))}\")\n",
    "    else:\n",
    "        print(\"Selected Classes: N/A\")\n",
    "    \n",
    "    generator_classes = metadata.get('generator_classes', None)\n",
    "    if generator_classes:\n",
    "        print(f\"Generator Classes: {', '.join(map(str, generator_classes))}\")\n",
    "    else:\n",
    "        print(\"Generator Classes: N/A\")\n",
    "    \n",
    "    # Section 6: State Dicts\n",
    "    print(\"\\n[6] STATE DICTIONARIES\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Has Generator State:  {metadata.get('has_generator_state', False)}\")\n",
    "    print(f\"Has Optimizer State:  {metadata.get('has_optimizer_state', False)}\")\n",
    "    \n",
    "    if metadata.get('has_generator_state'):\n",
    "        print(f\"\\nGenerator Layers ({len(metadata.get('parameter_keys', []))})\")\n",
    "        for i, key in enumerate(metadata.get('parameter_keys', [])[:10], 1):\n",
    "            print(f\"  {i}. {key}\")\n",
    "        if len(metadata.get('parameter_keys', [])) > 10:\n",
    "            print(f\"  ... and {len(metadata.get('parameter_keys', [])) - 10} more\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Inspect the first checkpoint\n",
    "if checkpoints:\n",
    "    inspect_checkpoint_detailed(checkpoints[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Custom Analysis\n",
    "\n",
    "Use this section for your own custom analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom analysis here\n",
    "# Example: Compare two specific checkpoints\n",
    "\n",
    "if len(checkpoints) >= 2:\n",
    "    print(\"Comparing first two checkpoints...\\n\")\n",
    "    \n",
    "    for i, ckpt_path in enumerate(checkpoints[:2], 1):\n",
    "        _, meta = load_checkpoint_metadata(ckpt_path)\n",
    "        print(f\"Checkpoint {i}: {Path(ckpt_path).name}\")\n",
    "        print(f\"  Round: {meta.get('round', 'N/A')}\")\n",
    "        print(f\"  Loss: {meta.get('final_loss', 'N/A')}\")\n",
    "        print(f\"  Samples: {meta.get('training_samples', 'N/A')}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
