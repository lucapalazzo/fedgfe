{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Query Examples\n",
    "\n",
    "Questo notebook mostra come interagire con Ollama in locale per:\n",
    "- Fare query ai modelli LLM\n",
    "- Generare embeddings\n",
    "- Usare modelli vision\n",
    "\n",
    "Documentazione: https://ollama.com/blog/embedding-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup e Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama API endpoint (default locale)\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Modelli disponibili sul sistema\n",
    "AVAILABLE_MODELS = [\n",
    "    \"qwen3-vl:32b\",          # Vision-Language model (20GB)\n",
    "    \"qwen3-vl:8b\",           # Vision-Language model più piccolo (6.1GB)\n",
    "    \"nomic-embed-text:v1.5\", # Embedding model (274MB)\n",
    "    \"qwen3:latest\",          # LLM (5.2GB)\n",
    "    \"deepseek-r1:8b\",        # LLM (5.2GB)\n",
    "    \"moondream:1.8b\",        # Vision model (1.7GB)\n",
    "    \"llava:13b\",             # Vision-Language model (8GB)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lista Modelli Disponibili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 models:\n",
      "\n",
      "  - x/flux2-klein:latest           (  5.33 GB) - Modified: 2026-01-21T09:09:00.331986544Z\n",
      "  - qwen3-vl:32b                   ( 19.47 GB) - Modified: 2026-01-20T17:26:48.395274756Z\n",
      "  - nomic-embed-text:v1.5          (  0.26 GB) - Modified: 2026-01-19T18:44:47.091835921Z\n",
      "  - moondream:1.8b                 (  1.62 GB) - Modified: 2026-01-11T00:15:36.591414258Z\n",
      "  - qwen3-vl:8b                    (  5.72 GB) - Modified: 2026-01-11T00:12:49.90249849Z\n",
      "  - nemotron-3-nano:30b            ( 22.61 GB) - Modified: 2025-12-26T11:40:18.119084142Z\n",
      "  - nemotron-3-nano:latest         ( 22.61 GB) - Modified: 2025-12-25T17:09:04.334692225Z\n",
      "  - llama4:16x17b                  ( 62.81 GB) - Modified: 2025-12-24T01:02:15.885941871Z\n",
      "  - llava:13b                      (  7.46 GB) - Modified: 2025-10-13T12:46:53.755449209Z\n",
      "  - qwen3:235b                     (132.39 GB) - Modified: 2025-09-24T18:16:10.146065174Z\n",
      "  - qwen3:latest                   (  4.87 GB) - Modified: 2025-09-24T18:01:23.51083967Z\n",
      "  - deepseek-r1:8b                 (  4.87 GB) - Modified: 2025-09-19T21:12:38.804241424Z\n",
      "  - llama4:scout                   ( 62.81 GB) - Modified: 2025-09-19T20:44:57.970037347Z\n",
      "  - gpt-oss:120b                   ( 60.81 GB) - Modified: 2025-09-19T20:00:59.271017981Z\n"
     ]
    }
   ],
   "source": [
    "def list_models():\n",
    "    \"\"\"Lista tutti i modelli disponibili in Ollama\"\"\"\n",
    "    response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get('models', [])\n",
    "        print(f\"Found {len(models)} models:\\n\")\n",
    "        for model in models:\n",
    "            name = model.get('name', 'Unknown')\n",
    "            size = model.get('size', 0) / (1024**3)  # Convert to GB\n",
    "            modified = model.get('modified_at', 'Unknown')\n",
    "            print(f\"  - {name:30s} ({size:6.2f} GB) - Modified: {modified}\")\n",
    "        return models\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Lista modelli\n",
    "models = list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query Semplice a un LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain what is federated learning in 2 sentences.\n",
      "\n",
      "Response:\n",
      "Federated learning is a machine learning approach where multiple devices or servers collaboratively train a model without sharing their data, enhancing privacy and security. The model is trained locally on each device, with only aggregated model updates shared centrally, allowing for collaborative learning without exposing raw data.\n"
     ]
    }
   ],
   "source": [
    "def query_llm(prompt: str, model: str = \"qwen3:latest\", stream: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Fa una query a un modello LLM.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Il prompt da inviare al modello\n",
    "        model: Nome del modello da usare\n",
    "        stream: Se True, stampa la risposta in streaming\n",
    "    \n",
    "    Returns:\n",
    "        La risposta completa del modello\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, stream=stream)\n",
    "    \n",
    "    if stream:\n",
    "        # Streaming mode\n",
    "        full_response = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                data = json.loads(line)\n",
    "                chunk = data.get('response', '')\n",
    "                print(chunk, end='', flush=True)\n",
    "                full_response += chunk\n",
    "                if data.get('done', False):\n",
    "                    break\n",
    "        print()  # New line\n",
    "        return full_response\n",
    "    else:\n",
    "        # Non-streaming mode\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result.get('response', '')\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "\n",
    "# Esempio di query\n",
    "prompt = \"Explain what is federated learning in 2 sentences.\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Response:\")\n",
    "response = query_llm(prompt, model=\"qwen3:latest\", stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chat Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(messages: List[Dict[str, str]], model: str = \"qwen3:latest\") -> str:\n",
    "    \"\"\"\n",
    "    Invia una conversazione multi-turn a un modello.\n",
    "    \n",
    "    Args:\n",
    "        messages: Lista di messaggi [{'role': 'user|assistant', 'content': 'text'}]\n",
    "        model: Nome del modello da usare\n",
    "    \n",
    "    Returns:\n",
    "        La risposta del modello\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/chat\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result.get('message', {}).get('content', '')\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}\"\n",
    "\n",
    "# Esempio di conversazione\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What is knowledge distillation?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Knowledge distillation is a technique where a smaller student model learns to mimic a larger teacher model.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you give me an example in federated learning?\"}\n",
    "]\n",
    "\n",
    "print(\"Conversation:\")\n",
    "for msg in conversation:\n",
    "    print(f\"{msg['role'].upper()}: {msg['content']}\")\n",
    "\n",
    "print(\"\\nASSISTANT:\", chat(conversation, model=\"qwen3:latest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for texts:\n",
      "  1. Federated learning is a distributed machine learning approach.\n",
      "  2. Knowledge distillation transfers knowledge from a teacher to a student model.\n",
      "  3. The weather is nice today.\n",
      "\n",
      "Generated embeddings shape: (3, 768)\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "def generate_embeddings(texts: List[str], model: str = \"nomic-embed-text:v1.5\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Genera embeddings per una lista di testi.\n",
    "    \n",
    "    Args:\n",
    "        texts: Lista di testi da embeddare\n",
    "        model: Modello di embedding da usare\n",
    "    \n",
    "    Returns:\n",
    "        Array numpy di embeddings (shape: [num_texts, embedding_dim])\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/embeddings\"\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for text in texts:\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": text\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            embedding = result.get('embedding', [])\n",
    "            embeddings.append(embedding)\n",
    "        else:\n",
    "            print(f\"Error for text '{text[:50]}...': {response.status_code}\")\n",
    "            embeddings.append(None)\n",
    "    \n",
    "    # Filter out None values and convert to numpy\n",
    "    embeddings = [e for e in embeddings if e is not None]\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Esempio di embedding generation\n",
    "texts = [\n",
    "    \"Federated learning is a distributed machine learning approach.\",\n",
    "    \"Knowledge distillation transfers knowledge from a teacher to a student model.\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "print(\"Generating embeddings for texts:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"  {i+1}. {text}\")\n",
    "\n",
    "embeddings = generate_embeddings(texts)\n",
    "print(f\"\\nGenerated embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calcola Similarità tra Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"Calcola la similarità coseno tra due vettori\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Calcola similarità tra i testi\n",
    "print(\"\\nSimilarity matrix:\")\n",
    "print(\"\\n\" + \" \" * 10 + \"Text 1   Text 2   Text 3\")\n",
    "for i in range(len(embeddings)):\n",
    "    similarities = []\n",
    "    for j in range(len(embeddings)):\n",
    "        sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "        similarities.append(sim)\n",
    "    print(f\"Text {i+1}:  {similarities[0]:.4f}   {similarities[1]:.4f}   {similarities[2]:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "sim_1_2 = cosine_similarity(embeddings[0], embeddings[1])\n",
    "sim_1_3 = cosine_similarity(embeddings[0], embeddings[2])\n",
    "print(f\"Similarity between Text 1 and Text 2 (both ML-related): {sim_1_2:.4f}\")\n",
    "print(f\"Similarity between Text 1 and Text 3 (unrelated): {sim_1_3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vision Model Query (con immagine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "def query_vision_model(prompt: str, image_path: str, model: str = \"qwen3-vl:8b\") -> str:\n",
    "    \"\"\"\n",
    "    Fa una query a un modello vision con un'immagine.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Il prompt testuale\n",
    "        image_path: Path all'immagine\n",
    "        model: Nome del modello vision da usare\n",
    "    \n",
    "    Returns:\n",
    "        La risposta del modello\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    \n",
    "    # Leggi e codifica l'immagine in base64\n",
    "    with open(image_path, 'rb') as img_file:\n",
    "        image_data = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"images\": [image_data],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result.get('response', '')\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}\"\n",
    "\n",
    "# Esempio (richiede un'immagine)\n",
    "# Decommentare e fornire un path valido\n",
    "# image_path = \"/path/to/your/image.jpg\"\n",
    "# if Path(image_path).exists():\n",
    "#     response = query_vision_model(\n",
    "#         prompt=\"Describe what you see in this image.\",\n",
    "#         image_path=image_path,\n",
    "#         model=\"qwen3-vl:8b\"\n",
    "#     )\n",
    "#     print(f\"Vision Model Response:\\n{response}\")\n",
    "# else:\n",
    "#     print(f\"Image not found: {image_path}\")\n",
    "\n",
    "print(\"Vision model query example (commented out - provide an image path to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Query per Federated Learning Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_query_batch(questions: List[str], model: str = \"qwen3:latest\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Esegue un batch di query di ricerca.\n",
    "    \n",
    "    Args:\n",
    "        questions: Lista di domande\n",
    "        model: Modello da usare\n",
    "    \n",
    "    Returns:\n",
    "        Dizionario {domanda: risposta}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n[{i}/{len(questions)}] Processing: {question}\")\n",
    "        response = query_llm(question, model=model, stream=False)\n",
    "        results[question] = response\n",
    "        print(f\"Response preview: {response[:150]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Esempio di batch query per ricerca FL\n",
    "research_questions = [\n",
    "    \"What are the main challenges in federated learning with heterogeneous data?\",\n",
    "    \"How does knowledge distillation help in federated learning scenarios?\",\n",
    "    \"Explain the concept of adapter modules in transfer learning.\"\n",
    "]\n",
    "\n",
    "print(\"Starting batch research queries...\")\n",
    "results = research_query_batch(research_questions, model=\"qwen3:latest\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF RESULTS\")\n",
    "print(\"=\"*80)\n",
    "for question, answer in results.items():\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {answer[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Semantic Search con Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, corpus: List[str], model: str = \"nomic-embed-text:v1.5\", top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Semantic search: trova i documenti più simili alla query.\n",
    "    \n",
    "    Args:\n",
    "        query: Testo della query\n",
    "        corpus: Lista di documenti\n",
    "        model: Modello di embedding\n",
    "        top_k: Numero di risultati da restituire\n",
    "    \n",
    "    Returns:\n",
    "        Lista di tuple (documento, similarità)\n",
    "    \"\"\"\n",
    "    # Genera embeddings per query e corpus\n",
    "    print(f\"Generating embeddings for query and {len(corpus)} documents...\")\n",
    "    query_embedding = generate_embeddings([query], model=model)[0]\n",
    "    corpus_embeddings = generate_embeddings(corpus, model=model)\n",
    "    \n",
    "    # Calcola similarità\n",
    "    similarities = []\n",
    "    for i, doc_embedding in enumerate(corpus_embeddings):\n",
    "        sim = cosine_similarity(query_embedding, doc_embedding)\n",
    "        similarities.append((corpus[i], sim))\n",
    "    \n",
    "    # Ordina per similarità decrescente\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Esempio di semantic search\n",
    "corpus_docs = [\n",
    "    \"Federated learning enables training models across decentralized data without sharing raw data.\",\n",
    "    \"Knowledge distillation compresses large neural networks into smaller ones.\",\n",
    "    \"Adapter modules allow efficient fine-tuning of pre-trained models.\",\n",
    "    \"The VEGAS dataset contains audio-visual data for multimodal learning.\",\n",
    "    \"Differential privacy protects individual data points in machine learning.\",\n",
    "    \"Generative models like VAE can create synthetic training samples.\",\n",
    "    \"Multi-modal learning combines information from different data modalities.\",\n",
    "]\n",
    "\n",
    "query = \"How to train models on distributed data?\"\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\\n\")\n",
    "print(\"Searching in corpus...\\n\")\n",
    "\n",
    "top_results = semantic_search(query, corpus_docs, top_k=3)\n",
    "\n",
    "print(\"\\nTop 3 most relevant documents:\")\n",
    "for i, (doc, similarity) in enumerate(top_results, 1):\n",
    "    print(f\"\\n{i}. [Similarity: {similarity:.4f}]\")\n",
    "    print(f\"   {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Utilità: Model Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info(model_name: str):\n",
    "    \"\"\"Ottiene informazioni dettagliate su un modello\"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/show\"\n",
    "    \n",
    "    payload = {\n",
    "        \"name\": model_name\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        info = response.json()\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if 'modelfile' in info:\n",
    "            print(f\"\\nModelfile:\\n{info['modelfile'][:500]}...\")\n",
    "        \n",
    "        if 'parameters' in info:\n",
    "            print(f\"\\nParameters: {info['parameters']}\")\n",
    "        \n",
    "        if 'template' in info:\n",
    "            print(f\"\\nTemplate: {info['template'][:200]}...\")\n",
    "        \n",
    "        return info\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Esempio: info su embedding model\n",
    "model_info = get_model_info(\"nomic-embed-text:v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Esempio Completo: RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question: str, knowledge_base: List[str], \n",
    "               embedding_model: str = \"nomic-embed-text:v1.5\",\n",
    "               llm_model: str = \"qwen3:latest\",\n",
    "               top_k: int = 2):\n",
    "    \"\"\"\n",
    "    RAG: Retrieval Augmented Generation\n",
    "    1. Cerca documenti rilevanti nella knowledge base\n",
    "    2. Usa i documenti come contesto per generare la risposta\n",
    "    \"\"\"\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    # Step 1: Retrieval\n",
    "    print(\"Step 1: Retrieving relevant documents...\")\n",
    "    relevant_docs = semantic_search(question, knowledge_base, \n",
    "                                   model=embedding_model, top_k=top_k)\n",
    "    \n",
    "    print(f\"Found {len(relevant_docs)} relevant documents\\n\")\n",
    "    \n",
    "    # Step 2: Augmentation (costruisci prompt con contesto)\n",
    "    context = \"\\n\".join([doc for doc, _ in relevant_docs])\n",
    "    \n",
    "    augmented_prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer the question based on the context provided above. If the context doesn't contain enough information, say so.\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"Step 2: Generating answer with context...\\n\")\n",
    "    \n",
    "    # Step 3: Generation\n",
    "    answer = query_llm(augmented_prompt, model=llm_model, stream=False)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FINAL ANSWER\")\n",
    "    print(\"=\"*80)\n",
    "    print(answer)\n",
    "    \n",
    "    return answer, relevant_docs\n",
    "\n",
    "# Knowledge base per federated learning\n",
    "fl_knowledge_base = [\n",
    "    \"Federated learning (FL) is a machine learning technique that trains models across multiple decentralized devices or servers holding local data samples, without exchanging them.\",\n",
    "    \"In federated learning, only model updates (gradients or parameters) are shared with a central server, preserving data privacy.\",\n",
    "    \"Knowledge distillation in FL allows a student model to learn from multiple teacher models distributed across nodes, improving generalization.\",\n",
    "    \"Adapter modules are small neural network components that can be fine-tuned while keeping the main model frozen, enabling efficient transfer learning.\",\n",
    "    \"The main challenges in FL include non-IID data distribution, communication costs, and model personalization.\",\n",
    "    \"Synthetic data generation using VAEs or GANs can help address data scarcity in federated settings.\",\n",
    "    \"Aggregation methods in FL include FedAvg (averaging model weights) and more advanced techniques like FedProx and SCAFFOLD.\",\n",
    "]\n",
    "\n",
    "# Esempio RAG\n",
    "question = \"How can knowledge distillation improve federated learning?\"\n",
    "answer, context_docs = rag_query(question, fl_knowledge_base, top_k=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONTEXT USED\")\n",
    "print(\"=\"*80)\n",
    "for i, (doc, sim) in enumerate(context_docs, 1):\n",
    "    print(f\"\\n{i}. [Similarity: {sim:.4f}]\")\n",
    "    print(f\"   {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note Finali\n",
    "\n",
    "### Modelli Disponibili:\n",
    "- **LLM**: `qwen3:latest`, `deepseek-r1:8b` - Per query generali\n",
    "- **Embedding**: `nomic-embed-text:v1.5` - Per semantic search e RAG\n",
    "- **Vision**: `qwen3-vl:8b`, `qwen3-vl:32b`, `llava:13b` - Per analisi immagini\n",
    "\n",
    "### API Endpoints:\n",
    "- `/api/generate` - Generazione testo\n",
    "- `/api/chat` - Conversazioni multi-turn\n",
    "- `/api/embeddings` - Generazione embeddings\n",
    "- `/api/tags` - Lista modelli\n",
    "- `/api/show` - Info modello\n",
    "\n",
    "### Documentazione:\n",
    "- https://ollama.com/blog/embedding-models\n",
    "- https://github.com/ollama/ollama/blob/main/docs/api.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
