{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Images from Saved Embeddings using FLUX\n",
    "\n",
    "This notebook loads T5 and CLIP embeddings from checkpoint files and uses FLUX to generate images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from diffusers import FluxPipeline\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Paths\nEMBEDDINGS_DIR = \"checkpoints/embeddings/vegas-2n-2c-400s-kd-debug\"\nOUTPUT_DIR = \"output_images/generated_from_embeddings\"\n\n# FLUX model configuration\nFLUX_MODEL = \"black-forest-labs/FLUX.1-schnell\"  # Fast version\n# Alternative: \"black-forest-labs/FLUX.1-dev\" for higher quality\n\n# Generation parameters\nNUM_INFERENCE_STEPS = 4  # Schnell optimized for 1-4 steps\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n\n# FIX: Use server embeddings which have the actual computed embeddings\n# Node embeddings have all None values\nUSE_SERVER_EMBEDDINGS = True\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"Dtype: {DTYPE}\")\nprint(f\"Embeddings directory: {EMBEDDINGS_DIR}\")\nprint(f\"Using server embeddings: {USE_SERVER_EMBEDDINGS}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FLUX Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading FLUX model: {FLUX_MODEL}...\")\n",
    "pipe = FluxPipeline.from_pretrained(\n",
    "    FLUX_MODEL,\n",
    "    torch_dtype=DTYPE\n",
    ")\n",
    "pipe = pipe.to(DEVICE)\n",
    "\n",
    "# Disable progress bar for cleaner output\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "print(\"✓ FLUX model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embedding Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_embedding_files(embeddings_dir, use_server_embeddings=True):\n    \"\"\"Load all .pt embedding files from directory.\"\"\"\n    if use_server_embeddings:\n        # Server embeddings have the actual computed embeddings\n        pattern = \"server_embeddings_*.pt\"\n    else:\n        # Node embeddings (might have None values)\n        pattern = \"node_*_embeddings_*.pt\"\n    \n    embedding_files = sorted(Path(embeddings_dir).glob(pattern))\n    \n    if not embedding_files:\n        raise FileNotFoundError(f\"No files matching '{pattern}' found in {embeddings_dir}\")\n    \n    print(f\"Found {len(embedding_files)} embedding files:\")\n    for f in embedding_files:\n        print(f\"  - {f.name}\")\n    \n    return embedding_files\n\nembedding_files = load_embedding_files(EMBEDDINGS_DIR, use_server_embeddings=USE_SERVER_EMBEDDINGS)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Embedding Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first file to inspect structure\n",
    "sample_file = embedding_files[0]\n",
    "print(f\"\\nInspecting: {sample_file.name}\")\n",
    "\n",
    "data = torch.load(sample_file, map_location='cpu')\n",
    "\n",
    "print(f\"\\nTop-level keys: {list(data.keys())}\")\n",
    "print(f\"Node ID: {data['node_id']}\")\n",
    "print(f\"Round: {data['round']}\")\n",
    "print(f\"Timestamp: {data['timestamp']}\")\n",
    "print(f\"Number of embeddings: {len(data['embeddings'])}\")\n",
    "\n",
    "if data['embeddings']:\n",
    "    print(f\"\\nFirst embedding keys: {list(data['embeddings'][0].keys())}\")\n",
    "    \n",
    "    # Check for non-None embeddings\n",
    "    valid_embeddings = [e for e in data['embeddings'] \n",
    "                       if e['t5_embedding'] is not None and e['clip_embedding'] is not None]\n",
    "    \n",
    "    print(f\"Valid embeddings (non-None): {len(valid_embeddings)}/{len(data['embeddings'])}\")\n",
    "    \n",
    "    if valid_embeddings:\n",
    "        sample_emb = valid_embeddings[0]\n",
    "        print(f\"\\nSample embedding info:\")\n",
    "        print(f\"  Class: {sample_emb['class_name']}\")\n",
    "        print(f\"  Split: {sample_emb['split']}\")\n",
    "        print(f\"  T5 shape: {sample_emb['t5_embedding'].shape}\")\n",
    "        print(f\"  CLIP shape: {sample_emb['clip_embedding'].shape}\")\n",
    "    else:\n",
    "        print(\"\\n⚠ Warning: No valid embeddings found (all are None)\")\n",
    "        print(\"This might happen if embeddings were saved in 'embeddings_only' mode\")\n",
    "        print(\"but the model wasn't run to generate the actual embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Images from Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_images_from_checkpoint(checkpoint_path, pipe, output_dir, max_images=None):\n    \"\"\"\n    Generate images from embeddings in a checkpoint file.\n    \n    Args:\n        checkpoint_path: Path to .pt checkpoint file\n        pipe: FluxPipeline instance\n        output_dir: Directory to save generated images\n        max_images: Maximum number of images to generate (None = all)\n    \n    Returns:\n        List of generated image paths\n    \"\"\"\n    # Load checkpoint\n    data = torch.load(checkpoint_path, map_location='cpu')\n    node_id = data.get('node_id', 'server')\n    round_num = data['round']\n    \n    # Filter valid embeddings\n    valid_embeddings = [\n        e for e in data['embeddings']\n        if e['t5_embedding'] is not None and e['clip_embedding'] is not None\n    ]\n    \n    if not valid_embeddings:\n        print(f\"⚠ No valid embeddings in {checkpoint_path.name}\")\n        return []\n    \n    # Limit number of images if specified\n    if max_images is not None:\n        valid_embeddings = valid_embeddings[:max_images]\n    \n    print(f\"\\nProcessing {checkpoint_path.name}:\")\n    print(f\"  Node/Server {node_id}, Round {round_num}\")\n    print(f\"  Generating {len(valid_embeddings)} images...\")\n    \n    generated_paths = []\n    \n    # Generate images\n    for i, emb_data in enumerate(tqdm(valid_embeddings, desc=\"Generating\")):\n        try:\n            # IMPORTANT FIX: The embeddings are stored with inverted names!\n            # In the checkpoint:\n            #   - 't5_embedding' actually contains CLIP embeddings [1, 768]\n            #   - 'clip_embedding' actually contains T5 embeddings [1, seq_len, 4096]\n            # \n            # FLUX expects:\n            #   - prompt_embeds: T5 embeddings [batch, seq_len, 4096]\n            #   - pooled_prompt_embeds: CLIP embeddings [batch, 768]\n            \n            # Get embeddings (with inverted names from checkpoint)\n            t5_saved = emb_data['t5_embedding']  # Actually CLIP [1, 768]\n            clip_saved = emb_data['clip_embedding']  # Actually T5 [1, seq_len, 4096]\n            \n            # Prepare embeddings with correct assignment for FLUX\n            prompt_embeds = clip_saved.to(DEVICE).to(DTYPE)  # T5: [1, seq_len, 4096]\n            pooled_prompt_embeds = t5_saved.to(DEVICE).to(DTYPE)  # CLIP: [1, 768]\n            \n            # Ensure correct shapes\n            if len(pooled_prompt_embeds.shape) == 2:\n                # Already [batch, 768] - good\n                pass\n            elif len(pooled_prompt_embeds.shape) == 1:\n                pooled_prompt_embeds = pooled_prompt_embeds.unsqueeze(0)\n            \n            if len(prompt_embeds.shape) == 2:\n                # If [seq_len, 4096], add batch dimension\n                prompt_embeds = prompt_embeds.unsqueeze(0)\n            \n            print(f\"  Image {i}: prompt_embeds={prompt_embeds.shape}, pooled={pooled_prompt_embeds.shape}\")\n            \n            # Generate image\n            result = pipe(\n                prompt_embeds=prompt_embeds,\n                pooled_prompt_embeds=pooled_prompt_embeds,\n                num_inference_steps=NUM_INFERENCE_STEPS,\n                output_type=\"pil\",\n                generator=torch.Generator(device=DEVICE).manual_seed(42 + i)  # Different seed per image\n            )\n            \n            image = result.images[0]\n            \n            # Save image\n            class_name = emb_data.get('class_name', 'unknown')\n            split = emb_data.get('split', 'global')\n            \n            filename = f\"server_r{round_num}_{split}_{class_name}_{i}.png\"\n            save_path = os.path.join(output_dir, filename)\n            \n            image.save(save_path)\n            generated_paths.append(save_path)\n            \n            # Clean up GPU memory\n            del prompt_embeds, pooled_prompt_embeds, result\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n                \n        except Exception as e:\n            print(f\"  ✗ Error generating image {i}: {e}\")\n            import traceback\n            traceback.print_exc()\n            continue\n    \n    print(f\"  ✓ Generated {len(generated_paths)} images\")\n    return generated_paths"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Images from All Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max_images_per_file to limit generation (useful for testing)\n",
    "MAX_IMAGES_PER_FILE = 10  # Set to None to generate all images\n",
    "\n",
    "all_generated_images = []\n",
    "\n",
    "for checkpoint_file in embedding_files:\n",
    "    generated = generate_images_from_checkpoint(\n",
    "        checkpoint_file,\n",
    "        pipe,\n",
    "        OUTPUT_DIR,\n",
    "        max_images=MAX_IMAGES_PER_FILE\n",
    "    )\n",
    "    all_generated_images.extend(generated)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total images generated: {len(all_generated_images)}\")\n",
    "print(f\"Saved to: {OUTPUT_DIR}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Sample Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 6 generated images\n",
    "num_display = min(6, len(all_generated_images))\n",
    "\n",
    "if num_display > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, img_path in enumerate(all_generated_images[:num_display]):\n",
    "        img = Image.open(img_path)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(Path(img_path).name, fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(num_display, 6):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No images to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Images from Specific Embeddings\n",
    "\n",
    "Use this section if you want to generate images from specific embeddings only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate from specific node and round\n",
    "target_node = 0\n",
    "target_round = 1\n",
    "\n",
    "target_file = Path(EMBEDDINGS_DIR) / f\"node_{target_node}_embeddings_r{target_round}.pt\"\n",
    "\n",
    "if target_file.exists():\n",
    "    print(f\"Generating from: {target_file.name}\")\n",
    "    generated = generate_images_from_checkpoint(\n",
    "        target_file,\n",
    "        pipe,\n",
    "        OUTPUT_DIR,\n",
    "        max_images=5  # Generate just 5 images\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    if generated:\n",
    "        fig, axes = plt.subplots(1, len(generated), figsize=(4*len(generated), 4))\n",
    "        if len(generated) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, img_path in zip(axes, generated):\n",
    "            img = Image.open(img_path)\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(Path(img_path).name, fontsize=8)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\nelse:\n",
    "    print(f\"File not found: {target_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generate with Custom Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate_images(embeddings_list, pipe, batch_size=4):\n",
    "    \"\"\"\n",
    "    Generate multiple images in batches for better efficiency.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_list: List of embedding dictionaries\n",
    "        pipe: FluxPipeline instance\n",
    "        batch_size: Number of images to generate at once\n",
    "    \n",
    "    Returns:\n",
    "        List of PIL Images\n",
    "    \"\"\"\n",
    "    all_images = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(embeddings_list), batch_size), desc=\"Batches\"):\n",
    "        batch = embeddings_list[i:i+batch_size]\n",
    "        \n",
    "        # Stack embeddings\n",
    "        prompt_embeds_batch = torch.stack([\n",
    "            e['t5_embedding'] for e in batch\n",
    "        ]).to(DEVICE).to(DTYPE)\n",
    "        \n",
    "        pooled_prompt_embeds_batch = torch.stack([\n",
    "            e['clip_embedding'] for e in batch\n",
    "        ]).to(DEVICE).to(DTYPE)\n",
    "        \n",
    "        # Generate batch\n",
    "        try:\n",
    "            result = pipe(\n",
    "                prompt_embeds=prompt_embeds_batch,\n",
    "                pooled_prompt_embeds=pooled_prompt_embeds_batch,\n",
    "                num_inference_steps=NUM_INFERENCE_STEPS,\n",
    "                output_type=\"pil\"\n",
    "            )\n",
    "            \n",
    "            all_images.extend(result.images)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i//batch_size}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Clean up\n",
    "        del prompt_embeds_batch, pooled_prompt_embeds_batch, result\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return all_images\n",
    "\n",
    "# Example usage:\n",
    "# Load embeddings from a file\n",
    "# data = torch.load(embedding_files[0], map_location='cpu')\n",
    "# valid_embs = [e for e in data['embeddings'] if e['t5_embedding'] is not None][:8]\n",
    "# images = batch_generate_images(valid_embs, pipe, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✓ GPU memory cleared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}